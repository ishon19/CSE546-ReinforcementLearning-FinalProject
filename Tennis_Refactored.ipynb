{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrBv_FKyktgC"
      },
      "source": [
        "**\"I/we certify that the code and data in this assignment were generated independently, using only the tools and resources defined in the course and that I/we did not receive any external help, coaching or contributions during the production of this work.\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02VNswxaeUAO"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "1_hJfh6EktgG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shreyans/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from unityagents import UnityEnvironment\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "from collections import deque\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjKIa-7We8Gy"
      },
      "source": [
        "# Class Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TSG2MLfTe_Vr"
      },
      "outputs": [],
      "source": [
        "class Constants:\n",
        "    \"\"\"Constants for the game.\"\"\"\n",
        "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    NN_DIM = 256\n",
        "    WEIGHT_NORM1 = 1e-2\n",
        "    WEIGHT_NORM2 = 3e-3\n",
        "    ALPHA_ACTOR = 1e-4\n",
        "    ALPHA_CRITIC = 1e-3\n",
        "    WEIGHT_DECAY = 0.0\n",
        "    ACTION_DIM = 2\n",
        "    SEED = 777\n",
        "    GAMMA = 0.99\n",
        "    TAU = 1e-3\n",
        "    ACTION_DIM = 2\n",
        "    BUFFER_SIZE = 10000\n",
        "    UPDATE_INTERVAL = 2\n",
        "    NOISE1 = 1.0\n",
        "    NOISE2 = 0.1\n",
        "    NOISE3 = 30000\n",
        "    STATE_DIM = 24\n",
        "    ACTION_DIM = 2\n",
        "    NUM_AGENTS = 2\n",
        "    BATCH_SIZE = 256\n",
        "    MU = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cCRTphl7rJHL"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, Constants.NN_DIM)\n",
        "        self.layer2 = nn.Linear(Constants.NN_DIM, Constants.NN_DIM)\n",
        "        self.layer3 = nn.Linear(Constants.NN_DIM, output_dim)\n",
        "        self.batch_norm = nn.BatchNorm1d(Constants.NN_DIM)\n",
        "\n",
        "        # normalize weights\n",
        "        self.layer1.weight.data.uniform_(-Constants.WEIGHT_NORM1,\n",
        "                                         Constants.WEIGHT_NORM1)\n",
        "        self.layer2.weight.data.uniform_(-Constants.WEIGHT_NORM1,\n",
        "                                         Constants.WEIGHT_NORM1)\n",
        "        self.layer3.weight.data.uniform_(-Constants.WEIGHT_NORM2,\n",
        "                                         Constants.WEIGHT_NORM2)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.layer1(state))\n",
        "        x = self.batch_norm(x)\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = torch.tanh(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, Constants.NN_DIM)\n",
        "        self.layer2 = nn.Linear(Constants.NN_DIM, Constants.NN_DIM)\n",
        "        self.layer3 = nn.Linear(Constants.NN_DIM, 1)\n",
        "\n",
        "        # normalize weights\n",
        "        self.layer1.weight.data.uniform_(-Constants.WEIGHT_NORM1,\n",
        "                                         Constants.WEIGHT_NORM1)\n",
        "        self.layer2.weight.data.uniform_(-Constants.WEIGHT_NORM1,\n",
        "                                         Constants.WEIGHT_NORM1)\n",
        "        self.layer3.weight.data.uniform_(-Constants.WEIGHT_NORM2,\n",
        "                                         Constants.WEIGHT_NORM2)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.layer1(state))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# critic model definition\n",
        "class ActorCritic():\n",
        "    def __init__(self):\n",
        "        # actor pair\n",
        "        self.ac_actor_local = Actor(\n",
        "            Constants.STATE_DIM, Constants.ACTION_DIM).to(Constants.DEVICE)\n",
        "        self.ac_actor_target = Actor(\n",
        "            Constants.STATE_DIM, Constants.ACTION_DIM).to(Constants.DEVICE)\n",
        "        input_size = (Constants.STATE_DIM +\n",
        "                      Constants.ACTION_DIM) * Constants.NUM_AGENTS\n",
        "\n",
        "        # critic pair\n",
        "        self.ac_critic_local = Critic(input_size).to(Constants.DEVICE)\n",
        "        self.ac_critic_target = Critic(input_size).to(Constants.DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1GCC0CBcrQrp"
      },
      "outputs": [],
      "source": [
        "class DDPG():\n",
        "    def __init__(self, idx):\n",
        "        self.mu = np.ones(Constants.ACTION_DIM) * Constants.MU\n",
        "        self.idx = idx\n",
        "        # the actor models\n",
        "        self.actor_local = Actor(Constants.ACTION_DIM,\n",
        "                                 Constants.NN_DIM).to(Constants.DEVICE)\n",
        "        self.actor_target = Actor(\n",
        "            Constants.ACTION_DIM, Constants.NN_DIM).to(Constants.DEVICE)\n",
        "\n",
        "        # the critic models\n",
        "        self.critic_local = Critic(Constants.NN_DIM).to(Constants.DEVICE)\n",
        "        self.critic_target = Critic(Constants.NN_DIM).to(Constants.DEVICE)\n",
        "\n",
        "        # set the optimizers\n",
        "        self.actor_optimizer = torch.optim.Adam(\n",
        "            self.actor_local.parameters(), lr=Constants.ALPHA_ACTOR)\n",
        "        self.critic_optimizer = torch.optim.Adam(\n",
        "            self.critic_local.parameters(), lr=Constants.ALPHA_CRITIC)\n",
        "\n",
        "        # noise generator\n",
        "        self.noise_state = copy.copy(Constants.MU)\n",
        "\n",
        "        # copy the weights from the local to the target\n",
        "        # critic\n",
        "        for target, local in zip(self.critic_target.parameters(), self.critic_local.parameters()):\n",
        "            target.data.copy_(local.data)\n",
        "\n",
        "        # actor\n",
        "        for target, local in zip(self.actor_target.parameters(), self.actor_local.parameters()):\n",
        "            target.data.copy_(local.data)\n",
        "\n",
        "    def sample_noise(self):\n",
        "        x, dx = self.noise_state, Constants.NOISE_THETA * \\\n",
        "            (self.mu - x) + Constants.NOISE_SIGMA * np.random.randn(self.size)\n",
        "        self.noise_state = x + dx\n",
        "        return self.noise_state\n",
        "\n",
        "    def _transform_state(self, state):\n",
        "        state_trans = torch.from_numpy(state).float().to(Constants.DEVICE)\n",
        "        self.actor_local.eval()\n",
        "        return state_trans\n",
        "\n",
        "    def action(self, state):\n",
        "        state = self._transform_state(state)\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "        self.actor_local.train()\n",
        "        self.val_noise = self.sample_noise() * Constants.NOISE1\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def _get_idx_actions(self, idx, actions_next):\n",
        "        agent_idx = torch.tensor([idx]).to(Constants.DEVICE)\n",
        "        next_actions = torch.cat(actions_next, dim=1).to(Constants.DEVICE)\n",
        "        return agent_idx, next_actions\n",
        "\n",
        "    def _get_q_values(self, idx, s, a,  r, d, target_q_next):\n",
        "        exp_q, tar_q = self.critic_local(s, a), r.index_select(\n",
        "            1, idx) + Constants.GAMMA * target_q_next * (1 - d.index_select(1, idx))\n",
        "        return exp_q, tar_q\n",
        "\n",
        "    def _get_expected_loss(self, s,  a, actions):\n",
        "        pred = [a.detach() if i != self.idx else a for i, a in actions]\n",
        "        pred = torch.cat(pred, dim=1)\n",
        "        return -self.critic_local(s, pred).mean()\n",
        "\n",
        "    def update(self, idx, experiences, actions_next, actions):\n",
        "        # get the states, actions, rewards, and don'ts from the experiences\n",
        "        s, a, r, n, d = experiences\n",
        "        self.critic.optimizer.zero_grad()\n",
        "        agent_idx, next_actions = self._get_idx_actions(idx, actions_next)\n",
        "\n",
        "        # get the Q values from the critic\n",
        "        with torch.no_grad():\n",
        "            target_q_next = self.critic_target(next_actions)\n",
        "        q_expected, q_targets = self._get_q_values(\n",
        "            idx, s, a, r, d, target_q_next)\n",
        "\n",
        "        # compute the critic loss\n",
        "        loss_critic = F.mse_loss(q_targets.detach(), q_expected)\n",
        "        loss_critic.backward()  # compute the gradients\n",
        "        self.critic_optimizer.step()  # update the critic\n",
        "\n",
        "        # compute the actor loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_actor = self._get_expected_loss(s, a, actions)\n",
        "        loss_actor.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # update the target networks\n",
        "        for target, local in zip(self.critic_target.parameters(), self.critic_local.parameters()):\n",
        "            target.data.copy_(Constants.TAU * local.data +\n",
        "                              (1 - Constants.TAU) * target.data)\n",
        "        for target, local in zip(self.actor_target.parameters(), self.actor_local.parameters()):\n",
        "            target.data.copy_(Constants.TAU * local.data +\n",
        "                              (1 - Constants.TAU) * target.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TONYka1Drvsv"
      },
      "outputs": [],
      "source": [
        "class MADDPGAgent():\n",
        "    def __init__(self):\n",
        "        np.random.seed(Constants.SEED)\n",
        "        random.seed(Constants.SEED)\n",
        "        self.steps = 0\n",
        "        self.model_list = [ActorCritic()] * Constants.NUM_AGENTS\n",
        "        self.agent_list = [DDPG(i) for i in range(Constants.NUM_AGENTS)]\n",
        "        self.experience = namedtuple(\"ReplayBuffer\", field_names=[\n",
        "                                        \"s\", \"a\", \"r\", \"n\", \"d\"])\n",
        "        self.memory = deque(maxlen=Constants.BUFFER_SIZE)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append(self.experience(\n",
        "            state, action, reward, next_state, done))\n",
        "\n",
        "    def _to_tensor(self, obj):\n",
        "        return torch.from_numpy(obj).float().to(Constants.DEVICE).float().to(Constants.DEVICE)\n",
        "\n",
        "    def sampleMemory(self):\n",
        "        memory_sample = random.sample(\n",
        "            self.memory, Constants.BATCH_SIZE)\n",
        "        s, a = self._to_tensor(\n",
        "            np.vstack([e.s for e in memory_sample if e is not None])),\n",
        "        self._to_tensor(\n",
        "            np.vstack([e.a for e in memory_sample if e is not None]))\n",
        "        r, n = self._to_tensor(\n",
        "            np.vstack([e.r for e in memory_sample if e is not None])),\n",
        "        self._to_tensor(\n",
        "            np.vstack([e.n for e in memory_sample if e is not None]))\n",
        "        d = self._to_tensor(\n",
        "            np.vstack([e.d for e in memory_sample if e is not None]))\n",
        "        return (s, a, r, n, d)\n",
        "    \n",
        "    def step(self, params):\n",
        "        state, action, reward, next_state, done = params\n",
        "        state = state.reshape((1, -1))\n",
        "        next_state = next_state.reshape((1, -1))\n",
        "        self.remember(state, action, reward, next_state, done)\n",
        "        self.steps += 1\n",
        "        if self.steps % Constants.UPDATE_INTERVAL == 0:\n",
        "            if len(self.memory) > Constants.BATCH_SIZE:\n",
        "                exps = [self.sampleMemory() for _ in range(Constants.NUM_AGENTS)]\n",
        "                self.update(exps)\n",
        "    \n",
        "    def action(self, states):\n",
        "        act_list = []\n",
        "        for a,s  in zip(self.agent_list, states):\n",
        "            action = a.action(s)\n",
        "            act_list.append(action)\n",
        "        return np.array(act_list).reshape((-1, 1))\n",
        "    \n",
        "    def update(self, exps):\n",
        "        next_act_list, act_list = [], []\n",
        "        for i, a in enumerate(self.agent_list):\n",
        "            s, a, r, n, d = exps[i]\n",
        "            a_idx = torch.tensor([i]).to(Constants.DEVICE)\n",
        "            state = s.reshape(1, 2, 24).index_select(1, a_idx).squeeze(1)\n",
        "            action = a.actor_local(state)\n",
        "            act_list.append(action) \n",
        "            next_state = n.reshape(1, 2, 24).index_select(1, a_idx).squeeze(1)\n",
        "            next_action = a.actor_target(next_state)\n",
        "            next_act_list.append(next_action)\n",
        "        \n",
        "        for i, a in enumerate(self.agent_list):\n",
        "            a.update((s, a, r, n, d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6LPtWOBfktgJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:unityagents:\n",
            "'Academy' started successfully!\n",
            "Unity Academy name: Academy\n",
            "        Number of Brains: 1\n",
            "        Number of External Brains : 1\n",
            "        Lesson number : 0\n",
            "        Reset Parameters :\n",
            "\t\t\n",
            "Unity brain name: TennisBrain\n",
            "        Number of Visual Observations (per agent): 0\n",
            "        Vector Observation space type: continuous\n",
            "        Vector Observation space size (per agent): 8\n",
            "        Number of stacked Vector Observation: 3\n",
            "        Vector Action space type: continuous\n",
            "        Vector Action space size (per agent): 2\n",
            "        Vector Action descriptions: , \n"
          ]
        }
      ],
      "source": [
        "env = UnityEnvironment(file_name='Tennis_Linux/Tennis.x86')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LQ2A-NacktgQ"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the default brain\n",
        "brain_name = env.brain_names[0]\n",
        "brain = env.brains[brain_name]\n",
        "env_info = env.reset(train_mode=True)[brain_name]\n",
        "num_agents = len(env_info.agents)\n",
        "action_size = brain.vector_action_space_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vAJOjgSfktgR",
        "outputId": "1d919c34-5816-4dcb-e421-e7ebe971a8dd"
      },
      "outputs": [],
      "source": [
        "class MultiAgentDDPG:\n",
        "  def __init__(self):\n",
        "    self.env = UnityEnvironment(file_name= \"Tennis_Linux/Tennis.x86_64\", no_graphics = True)\n",
        "    self.brain_name = self.env.brain_names[0]\n",
        "    self.brain = self.env.brains[self.brain_name]\n",
        "    self.n_episodes = 16000\n",
        "    self.max_t = 100\n",
        "    self.agent = MADDPGAgent()\n",
        "    self.scores_deque = deque(maxlen = 100)\n",
        "    self.scores_avg = []\n",
        "    self.scores = []\n",
        "\n",
        "  def train(self):\n",
        "    for i_episode in range(1, self.n_episodes + 1):\n",
        "      self.rewards = []\n",
        "      self.env_info = self.env.reset(train_mode = True)[self.brain_name]\n",
        "      self.state = self.env_info.vector_observations\n",
        "\n",
        "      for t in range(self.max_t):\n",
        "        self.action = self.agent.action(self.state)\n",
        "        self.env_info = self.env.step(self.action)[self.brain_name]\n",
        "        self.next_state = self.env_info.vector_observations\n",
        "        self.rewards_vec = self.env_info.rewards\n",
        "        self.done = self.env_info.local_done\n",
        "        self.agent.step((self.state, self.action, self.rewards_vec, self.next_state, self.done))\n",
        "        self.state = self.next_state\n",
        "        self.rewards.append(self.rewards_vec)\n",
        "        if any(self.done):\n",
        "          break\n",
        "        episode_reward = np.max(np.sum(np.array(self.rewards), axis = 0))\n",
        "        self.scores.append(episode_reward)\n",
        "        self.scores_deque.append(episode_reward)\n",
        "        current_avg_score = np.mean(self.scores_deque)\n",
        "        self.scores_avg.append(current_avg_score)\n",
        "      \n",
        "      print(\"Episode : \" + str(i_episode) + \", Average Score = \" + str(current_avg_score))\n",
        "      \n",
        "      if i_episode % 200 == 0:\n",
        "        print(\"Episode : \" + str(i_episode) + \", Average Score = \" + str(current_avg_score))\n",
        "        self.agent.save_agents()\n",
        "\n",
        "      if np.mean(self.scores_deque) >= 0.5:\n",
        "        print(\"Convergence achieved in \" + str(i_episode) + \" with Average score = \" + str(np.mean(self.scores_deque)))\n",
        "        self.agent.save_agents()      \n",
        "\n",
        "  def plot_results(self):\n",
        "    plt.figure(figsize = (10,5))\n",
        "    plt.plot(self.scores, color = \"lightseagreen\", linewidth = 3)\n",
        "    plt.xlabel('Episodes', fontsize = 17)\n",
        "    plt.ylabel('Score', fontsize = 17)\n",
        "    plt.title(f'Rewards per episode', fontsize = 21)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "  def close_env(self):\n",
        "    self.env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = MultiAgentDDPG()\n",
        "agent.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tennis_Refactored.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "633403847e8b7f924e6a934e1fdf7c9a73af5c662c1756552beaee0e4cef798b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
