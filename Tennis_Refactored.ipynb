{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrBv_FKyktgC"
      },
      "source": [
        "**\"I/we certify that the code and data in this assignment were generated independently, using only the tools and resources defined in the course and that I/we did not receive any external help, coaching or contributions during the production of this work.\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02VNswxaeUAO"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "1_hJfh6EktgG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shreyans/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from unityagents import UnityEnvironment\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "from collections import deque\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjKIa-7We8Gy"
      },
      "source": [
        "# Class Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TSG2MLfTe_Vr"
      },
      "outputs": [],
      "source": [
        "class Constants:\n",
        "    \"\"\"Constants for the game.\"\"\"\n",
        "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    NN_DIM = 256\n",
        "    WEIGHT_NORM1 = 1e-2\n",
        "    WEIGHT_NORM2 = 3e-3\n",
        "    ALPHA_ACTOR = 1e-4\n",
        "    ALPHA_CRITIC = 1e-3\n",
        "    WEIGHT_DECAY = 0.0\n",
        "    ACTION_DIM = 2\n",
        "    SEED = 777\n",
        "    GAMMA = 0.99\n",
        "    TAU = 1e-3\n",
        "    ACTION_DIM = 2\n",
        "    BUFFER_SIZE = 10000\n",
        "    UPDATE_INTERVAL = 2\n",
        "    NOISE1 = 1.0\n",
        "    NOISE2 = 0.1\n",
        "    NOISE3 = 30000\n",
        "    STATE_DIM = 24\n",
        "    ACTION_DIM = 2\n",
        "    NUM_AGENTS = 2\n",
        "    BATCH_SIZE = 256\n",
        "    MU = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cCRTphl7rJHL"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, Constants.NN_DIM)\n",
        "        self.layer2 = nn.Linear(Constants.NN_DIM, Constants.NN_DIM)\n",
        "        self.layer3 = nn.Linear(Constants.NN_DIM, output_dim)\n",
        "        self.batch_norm = nn.BatchNorm1d(Constants.NN_DIM)\n",
        "\n",
        "        # normalize weights\n",
        "        self.layer1.weight.data.uniform_(-Constants.WEIGHT_NORM1,\n",
        "                                         Constants.WEIGHT_NORM1)\n",
        "        self.layer2.weight.data.uniform_(-Constants.WEIGHT_NORM1,\n",
        "                                         Constants.WEIGHT_NORM1)\n",
        "        self.layer3.weight.data.uniform_(-Constants.WEIGHT_NORM2,\n",
        "                                         Constants.WEIGHT_NORM2)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.layer1(state))\n",
        "        x = self.batch_norm(x)\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = torch.tanh(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, Constants.NN_DIM)\n",
        "        self.layer2 = nn.Linear(Constants.NN_DIM, Constants.NN_DIM)\n",
        "        self.layer3 = nn.Linear(Constants.NN_DIM, 1)\n",
        "\n",
        "        # normalize weights\n",
        "        self.layer1.weight.data.uniform_(-Constants.WEIGHT_NORM1,\n",
        "                                         Constants.WEIGHT_NORM1)\n",
        "        self.layer2.weight.data.uniform_(-Constants.WEIGHT_NORM1,\n",
        "                                         Constants.WEIGHT_NORM1)\n",
        "        self.layer3.weight.data.uniform_(-Constants.WEIGHT_NORM2,\n",
        "                                         Constants.WEIGHT_NORM2)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.layer1(state))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# critic model definition\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        # actor pair\n",
        "        self.ac_actor_local = Actor(\n",
        "            Constants.STATE_DIM, Constants.ACTION_DIM).to(Constants.DEVICE)\n",
        "        self.ac_actor_target = Actor(\n",
        "            Constants.STATE_DIM, Constants.ACTION_DIM).to(Constants.DEVICE)\n",
        "        input_size = (Constants.STATE_DIM +\n",
        "                      Constants.ACTION_DIM) * Constants.NUM_AGENTS\n",
        "\n",
        "        # critic pair\n",
        "        self.ac_critic_local = Critic(input_size).to(Constants.DEVICE)\n",
        "        self.ac_critic_target = Critic(input_size).to(Constants.DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1GCC0CBcrQrp"
      },
      "outputs": [],
      "source": [
        "class DDPG():\n",
        "    def __init__(self, idx):\n",
        "        self.mu = np.ones(Constants.ACTION_DIM) * Constants.MU\n",
        "        self.idx = idx\n",
        "        # the actor models\n",
        "        self.actor_local = Actor(Constants.ACTION_DIM,\n",
        "                                 Constants.NN_DIM).to(Constants.DEVICE)\n",
        "        self.actor_target = Actor(\n",
        "            Constants.ACTION_DIM, Constants.NN_DIM).to(Constants.DEVICE)\n",
        "\n",
        "        # the critic models\n",
        "        self.critic_local = Critic(Constants.NN_DIM).to(Constants.DEVICE)\n",
        "        self.critic_target = Critic(Constants.NN_DIM).to(Constants.DEVICE)\n",
        "\n",
        "        # set the optimizers\n",
        "        self.actor_optimizer = torch.optim.Adam(\n",
        "            self.actor_local.parameters(), lr=Constants.ALPHA_ACTOR)\n",
        "        self.critic_optimizer = torch.optim.Adam(\n",
        "            self.critic_local.parameters(), lr=Constants.ALPHA_CRITIC)\n",
        "\n",
        "        # noise generator\n",
        "        self.noise_state = copy.copy(Constants.MU)\n",
        "\n",
        "        # copy the weights from the local to the target\n",
        "        # critic\n",
        "        for target, local in zip(self.critic_target.parameters(), self.critic_local.parameters()):\n",
        "            target.data.copy_(local.data)\n",
        "\n",
        "        # actor\n",
        "        for target, local in zip(self.actor_target.parameters(), self.actor_local.parameters()):\n",
        "            target.data.copy_(local.data)\n",
        "\n",
        "    def sample_noise(self):\n",
        "        x, dx = self.noise_state, Constants.NOISE_THETA * \\\n",
        "            (self.mu - x) + Constants.NOISE_SIGMA * np.random.randn(self.size)\n",
        "        self.noise_state = x + dx\n",
        "        return self.noise_state\n",
        "\n",
        "    def _transform_state(self, state):\n",
        "        state_trans = torch.from_numpy(state).float().to(Constants.DEVICE)\n",
        "        self.actor_local.eval()\n",
        "        return state_trans\n",
        "\n",
        "    def action(self, state):\n",
        "        state = self._transform_state(state)\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "        self.actor_local.train()\n",
        "        self.val_noise = self.sample_noise() * Constants.NOISE1\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def _get_idx_actions(self, idx, actions_next):\n",
        "        agent_idx = torch.tensor([idx]).to(Constants.DEVICE)\n",
        "        next_actions = torch.cat(actions_next, dim=1).to(Constants.DEVICE)\n",
        "        return agent_idx, next_actions\n",
        "\n",
        "    def _get_q_values(self, idx, s, a,  r, d, target_q_next):\n",
        "        exp_q, tar_q = self.critic_local(s, a), r.index_select(\n",
        "            1, idx) + Constants.GAMMA * target_q_next * (1 - d.index_select(1, idx))\n",
        "        return exp_q, tar_q\n",
        "\n",
        "    def _get_expected_loss(self, s,  a, actions):\n",
        "        pred = [a.detach() if i != self.idx else a for i, a in actions]\n",
        "        pred = torch.cat(pred, dim=1)\n",
        "        return -self.critic_local(s, pred).mean()\n",
        "\n",
        "    def update(self, idx, experiences, actions_next, actions):\n",
        "        # get the states, actions, rewards, and don'ts from the experiences\n",
        "        s, a, r, n, d = experiences\n",
        "        self.critic.optimizer.zero_grad()\n",
        "        agent_idx, next_actions = self._get_idx_actions(idx, actions_next)\n",
        "\n",
        "        # get the Q values from the critic\n",
        "        with torch.no_grad():\n",
        "            target_q_next = self.critic_target(next_actions)\n",
        "        q_expected, q_targets = self._get_q_values(\n",
        "            idx, s, a, r, d, target_q_next)\n",
        "\n",
        "        # compute the critic loss\n",
        "        loss_critic = F.mse_loss(q_targets.detach(), q_expected)\n",
        "        loss_critic.backward()  # compute the gradients\n",
        "        self.critic_optimizer.step()  # update the critic\n",
        "\n",
        "        # compute the actor loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        loss_actor = self._get_expected_loss(s, a, actions)\n",
        "        loss_actor.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # update the target networks\n",
        "        for target, local in zip(self.critic_target.parameters(), self.critic_local.parameters()):\n",
        "            target.data.copy_(Constants.TAU * local.data +\n",
        "                              (1 - Constants.TAU) * target.data)\n",
        "        for target, local in zip(self.actor_target.parameters(), self.actor_local.parameters()):\n",
        "            target.data.copy_(Constants.TAU * local.data +\n",
        "                              (1 - Constants.TAU) * target.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TONYka1Drvsv"
      },
      "outputs": [],
      "source": [
        "class MADDPGAgent():\n",
        "    def __init__(self):\n",
        "        np.random.seed(Constants.SEED)\n",
        "        random.seed(Constants.SEED)\n",
        "        self.steps = 0\n",
        "        model_list = [ActorCritic()] * Constants.NUM_AGENTS\n",
        "        agent_list = [DDPG(i) for i in range(Constants.NUM_AGENTS)]\n",
        "        self.experience = namedtuple(\"ReplayBuffer\", field_names=[\n",
        "                                        \"s\", \"a\", \"r\", \"n\", \"d\"])\n",
        "        self.memory = deque(maxlen=Constants.BUFFER_SIZE)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append(self.experience(\n",
        "            state, action, reward, next_state, done))\n",
        "\n",
        "    def _to_tensor(self, obj):\n",
        "        return torch.from_numpy(obj).float().to(Constants.DEVICE).float().to(Constants.DEVICE)\n",
        "\n",
        "    def sampleMemory(self):\n",
        "        memory_sample = random.sample(\n",
        "            self.memory, Constants.BATCH_SIZE)\n",
        "        s, a = self._to_tensor(\n",
        "            np.vstack([e.s for e in memory_sample if e is not None])),\n",
        "        self._to_tensor(\n",
        "            np.vstack([e.a for e in memory_sample if e is not None]))\n",
        "        r, n = self._to_tensor(\n",
        "            np.vstack([e.r for e in memory_sample if e is not None])),\n",
        "        self._to_tensor(\n",
        "            np.vstack([e.n for e in memory_sample if e is not None]))\n",
        "        d = self._to_tensor(\n",
        "            np.vstack([e.d for e in memory_sample if e is not None]))\n",
        "        return (s, a, r, n, d)\n",
        "    \n",
        "    def step(self, params):\n",
        "        state, action, reward, next_state, done = params\n",
        "        state = state.reshape((1, -1))\n",
        "        next_state = next_state.reshape((1, -1))\n",
        "        self.remember(state, action, reward, next_state, done)\n",
        "        self.steps += 1\n",
        "        if self.steps % Constants.UPDATE_INTERVAL == 0:\n",
        "            if len(self.memory) > Constants.BATCH_SIZE:\n",
        "                exps = [self.sampleMemory() for _ in range(Constants.NUM_AGENTS)]\n",
        "                self.update(exps)\n",
        "    \n",
        "    def action(self, states):\n",
        "        act_list = []\n",
        "        for a,s  in zip(self.agent_list, states):\n",
        "            action = a.action(s)\n",
        "            act_list.append(action)\n",
        "        return np.array(act_list).reshape((-1, 1))\n",
        "    \n",
        "    def update(self, exps):\n",
        "        next_act_list, act_list = [], []\n",
        "        for i, a in enumerate(self.agent_list):\n",
        "            s, a, r, n, d = exps[i]\n",
        "            a_idx = torch.tensor([i]).to(Constants.DEVICE)\n",
        "            state = s.reshape(1, 2, 24).index_select(1, a_idx).squeeze(1)\n",
        "            action = a.actor_local(state)\n",
        "            act_list.append(action) \n",
        "            next_state = n.reshape(1, 2, 24).index_select(1, a_idx).squeeze(1)\n",
        "            next_action = a.actor_target(next_state)\n",
        "            next_act_list.append(next_action)\n",
        "        \n",
        "        for i, a in enumerate(self.agent_list):\n",
        "            a.update((s, a, r, n, d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6LPtWOBfktgJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:unityagents:\n",
            "'Academy' started successfully!\n",
            "Unity Academy name: Academy\n",
            "        Number of Brains: 1\n",
            "        Number of External Brains : 1\n",
            "        Lesson number : 0\n",
            "        Reset Parameters :\n",
            "\t\t\n",
            "Unity brain name: TennisBrain\n",
            "        Number of Visual Observations (per agent): 0\n",
            "        Vector Observation space type: continuous\n",
            "        Vector Observation space size (per agent): 8\n",
            "        Number of stacked Vector Observation: 3\n",
            "        Vector Action space type: continuous\n",
            "        Vector Action space size (per agent): 2\n",
            "        Vector Action descriptions: , \n"
          ]
        }
      ],
      "source": [
        "env = UnityEnvironment(file_name='Tennis_Linux/Tennis.x86')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ2A-NacktgQ"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the default brain\n",
        "brain_name = env.brain_names[0]\n",
        "brain = env.brains[brain_name]\n",
        "env_info = env.reset(train_mode=True)[brain_name]\n",
        "num_agents = len(env_info.agents)\n",
        "action_size = brain.vector_action_space_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vAJOjgSfktgR",
        "outputId": "1d919c34-5816-4dcb-e421-e7ebe971a8dd"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "cannot assign module before Module.__init__() call",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m MADDPGAgent()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000011vscode-remote?line=1'>2</a>\u001b[0m n_episodes \u001b[39m=\u001b[39m \u001b[39m6000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000011vscode-remote?line=2'>3</a>\u001b[0m max_t \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n",
            "\u001b[1;32m/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb Cell 8'\u001b[0m in \u001b[0;36mMADDPGAgent.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000008vscode-remote?line=3'>4</a>\u001b[0m random\u001b[39m.\u001b[39mseed(Constants\u001b[39m.\u001b[39mSEED)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m model_list \u001b[39m=\u001b[39m [ActorCritic()] \u001b[39m*\u001b[39m Constants\u001b[39m.\u001b[39mNUM_AGENTS\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000008vscode-remote?line=6'>7</a>\u001b[0m agent_list \u001b[39m=\u001b[39m [DDPG(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Constants\u001b[39m.\u001b[39mNUM_AGENTS)]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperience \u001b[39m=\u001b[39m namedtuple(\u001b[39m\"\u001b[39m\u001b[39mReplayBuffer\u001b[39m\u001b[39m\"\u001b[39m, field_names\u001b[39m=\u001b[39m[\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m                                 \u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "\u001b[1;32m/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb Cell 6'\u001b[0m in \u001b[0;36mActorCritic.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000006vscode-remote?line=48'>49</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000006vscode-remote?line=49'>50</a>\u001b[0m     \u001b[39m# actor pair\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000006vscode-remote?line=50'>51</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mac_actor_local \u001b[39m=\u001b[39m Actor(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000006vscode-remote?line=51'>52</a>\u001b[0m         Constants\u001b[39m.\u001b[39mSTATE_DIM, Constants\u001b[39m.\u001b[39mACTION_DIM)\u001b[39m.\u001b[39mto(Constants\u001b[39m.\u001b[39mDEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000006vscode-remote?line=52'>53</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mac_actor_target \u001b[39m=\u001b[39m Actor(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000006vscode-remote?line=53'>54</a>\u001b[0m         Constants\u001b[39m.\u001b[39mSTATE_DIM, Constants\u001b[39m.\u001b[39mACTION_DIM)\u001b[39m.\u001b[39mto(Constants\u001b[39m.\u001b[39mDEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000006vscode-remote?line=54'>55</a>\u001b[0m     input_size \u001b[39m=\u001b[39m (Constants\u001b[39m.\u001b[39mSTATE_DIM \u001b[39m+\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shreyans/UB/RL/MARL/final-project/Tennis_Refactored.ipynb#ch0000006vscode-remote?line=55'>56</a>\u001b[0m                   Constants\u001b[39m.\u001b[39mACTION_DIM) \u001b[39m*\u001b[39m Constants\u001b[39m.\u001b[39mNUM_AGENTS\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:976\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    <a href='file:///home/shreyans/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=973'>974</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Module):\n\u001b[1;32m    <a href='file:///home/shreyans/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=974'>975</a>\u001b[0m     \u001b[39mif\u001b[39;00m modules \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/shreyans/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=975'>976</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/shreyans/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=976'>977</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcannot assign module before Module.__init__() call\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/shreyans/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=977'>978</a>\u001b[0m     remove_from(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_non_persistent_buffers_set)\n\u001b[1;32m    <a href='file:///home/shreyans/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=978'>979</a>\u001b[0m     modules[name] \u001b[39m=\u001b[39m value\n",
            "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
          ]
        }
      ],
      "source": [
        "agent = MADDPGAgent()\n",
        "n_episodes = 6000\n",
        "max_t = 1000\n",
        "scores = []\n",
        "scores_deque = deque(maxlen=100)\n",
        "scores_avg = []\n",
        "\n",
        "for i_episode in range(1, n_episodes+1):\n",
        "    rewards = []\n",
        "    env_info = env.reset(train_mode=False)[brain_name]    # reset the environment    \n",
        "    state = env_info.vector_observations                  # get the current state (for each agent)\n",
        "\n",
        "    # loop over steps\n",
        "    for t in range(max_t):\n",
        "        # select an action\n",
        "        action = agent.act(state)\n",
        "        # take action in environment and set parameters to new values\n",
        "        env_info = env.step(action)[brain_name]\n",
        "        next_state = env_info.vector_observations\n",
        "        rewards_vec = env_info.rewards\n",
        "        done = env_info.local_done\n",
        "        # update and train agent with returned information\n",
        "        agent.step(state, action, rewards_vec, next_state, done)\n",
        "        state = next_state\n",
        "        rewards.append(rewards_vec)\n",
        "        if any(done):\n",
        "            break\n",
        "\n",
        "    # calculate episode reward as maximum of individually collected rewards of agents\n",
        "    episode_reward = np.max(np.sum(np.array(rewards),axis=0))\n",
        "        \n",
        "    scores.append(episode_reward)             # save most recent score to overall score array\n",
        "    scores_deque.append(episode_reward)       # save most recent score to running window of 100 last scores\n",
        "    current_avg_score = np.mean(scores_deque)\n",
        "    scores_avg.append(current_avg_score)      # save average of last 100 scores to average score array\n",
        "    \n",
        "    print('\\rEpisode {}\\tAverage Score: {:.3f}'.format(i_episode, current_avg_score),end=\"\")\n",
        "    \n",
        "    # log average score every 200 episodes\n",
        "    if i_episode % 200 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.3f}'.format(i_episode, current_avg_score))\n",
        "        agent.save_agents()\n",
        "\n",
        "    # break and report success if environment is solved\n",
        "    if np.mean(scores_deque)>=.5:\n",
        "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode, np.mean(scores_deque)))\n",
        "        agent.save_agents()\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tennis_Refactored.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "633403847e8b7f924e6a934e1fdf7c9a73af5c662c1756552beaee0e4cef798b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
